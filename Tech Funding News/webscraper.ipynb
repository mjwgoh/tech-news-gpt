{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import URL library to access URLs online"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "search_website = \"URL HERE\"\n",
    "req = requests.get(search_website)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test print"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_results = soup.find_all(\"div\", class_=\"search-result\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_results = str(search_results)\n",
    "search_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use OpenAI to retrieve results that are relevant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "\n",
    "import os ## import os allows you to access operating system functions\n",
    "from dotenv import load_dotenv, find_dotenv #used to locate the .env file\n",
    "_ = load_dotenv(find_dotenv()) # executes the two functions in sequence. underscore (_) as a convention to indicate that the return value of load_dotenv() is not being used or assigned to a variable. makes variables in .env file available throughout.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import langchain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "llm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = open(\"webscraping_prompt.txt\", \"r\")\n",
    "webscraping_prompt = data.read()\n",
    "data.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert HTML file into text file to improve subsequent prompt engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_articles = llm.predict(\"Convert the HTML file into a text output with the following format: \\n 1. Article Title \\n 2. Article URL \\n 3. Article Published Date: \\n 4. Article Description \\n HTML file:\" + \"\\n\" + search_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"articles.txt\", \"w\") as f:\n",
    "    f.write(cleaned_articles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cutoffDate() :\n",
    "    import datetime\n",
    "    import dateutil.relativedelta\n",
    "    today = datetime.date.today()\n",
    "    cutoff = today - dateutil.relativedelta.relativedelta(days=30)\n",
    "    cutoff = cutoff.strftime(\"%d\" + \" \" + \"%B\" + \" \" + \"%Y\")\n",
    "\n",
    "    return cutoff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = open(\"articles.txt\", \"r\")\n",
    "search_results = data.read()\n",
    "data.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scrape_processed = webscraping_prompt + \"\\n 2. Only articles generated after \" + cutoffDate() + \" should be included in the response.\" + \"\\n\" + \"user:\" + \"\\n\" + search_results\n",
    "scrape_processed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "websites = llm.predict(scrape_processed)\n",
    "websites"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('websites.json', \"w\") as f:\n",
    "    f.write(websites)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read JSON file as object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Begin prompt chaining to remove non-startups"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Begin langchain step-wise checks\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "lang_llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=lang_llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation.predict(input = \"For each article in the JSON input, if the company mentioned in the article raised funding directly, append 'Funding raised: Yes' to the JSON input. If it was not the company that raised funding directly (i.e. it was a participant, partner, or investee), append 'Funding raised: No' to the input. Your response should be strictly a JSON file.\" + \"\\n\" + websites)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "startups_clean = conversation.predict(input = \"If 'Funding raised' is 'No' for the article, remove it from the JSON input. Return the revised JSON input as your response.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save output as JSON file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "startups_clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"websites.json\", \"w\") as f:\n",
    "    f.write(startups_clean)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load JSON file as Dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = open(\"websites.json\", \"r\")\n",
    "startups_clean_json = dict(json.load(data))\n",
    "data.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "startups_clean_json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterate through dict and retrieve articles related to each startup in the dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles_search_cleaned = []\n",
    "\n",
    "for article_details in startups_clean_json['articles']:\n",
    "    articles_search_cleaned.append(article_details[\"company\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles_search_cleaned"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_articles = []\n",
    "\n",
    "for search_terms in articles_search_cleaned:\n",
    "\n",
    "    req = requests.get(\"SEARCH URL HERE\") #Insert the search URL here\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    search_results = soup.find_all(\"div\", class_=\"search-result\")\n",
    "\n",
    "    output_articles.append([str(search_terms), search_results])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_articles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for article in output_articles:\n",
    "    temp_export = llm.predict(\"Using information in the HTML file below, extract all article URLs and respond in the format of a comma separated value file of format: Article 1 URL, Article 2 URL, .... \\n Include a maximum of the 3 latest articles in your response. \\n HTML file:\" + \"\\n\" + str(article[1]))\n",
    "\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    file_name = article[0] + \".json\"\n",
    "    file_path = os.path.join(\"Scraped Articles\", file_name)\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(temp_export)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract and save all articles to the same list. Export the list as the final text file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Function to obtain articles in a given list\n",
    "\n",
    "def extract_article(input_list):\n",
    "\n",
    "    counter = 0\n",
    "    output_string = \"\"\n",
    "    temp_output = \"\"\n",
    "\n",
    "    for url in input_list:\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        temp_request = requests.get(str(url))\n",
    "        temp_article = BeautifulSoup(temp_request.text, \"html.parser\")\n",
    "\n",
    "        temp_byline = temp_article.find_all(\"p\", class_=\"byline\")\n",
    "        temp_text = temp_article.find_all(\"div\", class_=\"post\")\n",
    "\n",
    "        output_string = str(temp_byline)[1:-1] + \"\\n\" + str(temp_text)[1:-1]\n",
    "\n",
    "        prompt_article_extraction = \"system: Convert the HTML content in the user input into plain text. Your response must be in the format of: \\n Date: Published Date \\n Content: Article content body \\n You should not provide any additional information beyond the above stipulated format. \\n user: \\n\" + output_string\n",
    "\n",
    "        output_string = llm.predict(prompt_article_extraction)\n",
    "\n",
    "        temp_output = temp_output + \"Article \" + str(counter) + \"\\n\" + output_string + \"\\n\" + \"\\n\" + \"\\n\"\n",
    "\n",
    "    return temp_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"Scraped Articles\"\n",
    "\n",
    "# Get a list of files in the folder\n",
    "files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for file_name in files_in_folder:\n",
    "\n",
    "    file_path = os.path.join(folder_path, str(file_name))\n",
    "\n",
    "    data = open(file_path, \"r\")\n",
    "    temp_list = list(data.read().split(\",\"))\n",
    "    data.close()\n",
    "\n",
    "    temp_extracted_article = extract_article(temp_list)\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(temp_extracted_article)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
